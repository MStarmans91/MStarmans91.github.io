<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Radiomics | Martijn P.A. Starmans, PhD - PI AI for Integrated Diagnostics (AIID)</title>
    <link>https://MStarmans91.github.io/tag/radiomics/</link>
      <atom:link href="https://MStarmans91.github.io/tag/radiomics/index.xml" rel="self" type="application/rss+xml" />
    <description>Radiomics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 02 Jun 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://MStarmans91.github.io/media/icon_hu08ff99a11f940ac4761bcab7c4c5c317_13983_512x512_fill_lanczos_center_3.png</url>
      <title>Radiomics</title>
      <link>https://MStarmans91.github.io/tag/radiomics/</link>
    </image>
    
    <item>
      <title>Automated Deep Learning for Soft-Tissue Tumor Classification Using Meta-Learning and AutoML</title>
      <link>https://MStarmans91.github.io/project/automlmeta/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://MStarmans91.github.io/project/automlmeta/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AI models in medical imaging are usually developed from scratch using disease-specific datasets. While this works for common diseases, it poses a major bottleneck for rare cancers like soft-tissue tumors (STTs), where labelled data is scarce. Unlike humans, who can learn new concepts from just a few examples by drawing on prior experience, current AI systems require large amounts of task-specific data due to how they are trained. This project addresses that limitation by combining meta-learning and automated machine learning (AutoML) to enable knowledge transfer across clinical and reducing reliance on large datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aim&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aim The aim of this project is to develop a novel methodology that integrates automated machine learning and meta-learning for medical image classification. Instead of trial-and-error model design, we will automate model selection and tuning using prior experience on related tasks and datasets. By learning from previous clinical applications, the system will be able to generalize across different diseases and imaging modalities, particularly in the context of rare cancers where data is limited. The project will involve designing model search strategies, incorporating knowledge transfer mechanisms, and evaluating the approach on real-world clinical datasets. Note that this project is quite technical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related research:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hutter, F., Kotthoff, L., &amp;amp; Vanschoren, J. (Eds.). (2019). Automated machine learning: Methods, systems, challenges. Springer.&lt;/li&gt;
&lt;li&gt;Hospedales, T., Antoniou, A., Micaelli, P., &amp;amp; Storkey, A. (2021). Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9), 5149-5169.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Supervisors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Natalia Oviedo Acosta&lt;/li&gt;
&lt;li&gt;Stefan Klein&lt;/li&gt;
&lt;li&gt;Martijn Starmans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Feel free to mail me if you are interested in this project or want more information!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking AutoML and Meta-Learning on Medical Image Classification</title>
      <link>https://MStarmans91.github.io/project/automlbenchmark/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://MStarmans91.github.io/project/automlbenchmark/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Medical image classification has advanced rapidly due to the availability of large pretrained models and foundation models. However, most model development workflows in medical imaging are still highly customized and dataset-specific, making it difficult to compare methods or reproduce results. This lack of standardization is especially limiting for techniques like automated machine learning (AutoML) and meta-learning, which aim to generalize across tasks and reduce manual effort. While generic AutoML benchmarks exist for natural images or tabular data, there is no established benchmark tailored to the unique characteristics of medical image data particularly in oncology. Creating such a benchmark would enable fair, systematic evaluation of AutoML and meta-learning approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aim&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This project aims to build a comprehensive and extensible benchmark for evaluating AutoML and meta-learning methods in medical image classification, with a particular emphasis on oncological tasks. It involves curating a diverse collection of publicly available medical imaging datasets, prioritizing cancer-related classification problems across modalities such as CT and MRI. In parallel, a portfolio of pretrained and foundation models—including resources such as SAM-Med3D [1], MONAI Model Zoo, and others—will be collected and structured into a unified, reusable model hub. Based on these components, the benchmark will support the systematic evaluation of AutoML and meta-learning frameworks, including methods like Quick-Tune [2] and DEHB [3]. Evaluations will focus on metrics such as classification accuracy, cross-dataset generalization, and computational efficiency. Note that this project is quite technical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related research:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wang, S., Zhu, C., Lu, M. Y., Qian, Z., Chen, R. J., &amp;amp; Mahmood, F. (2023). SAM-Med3D: Segment Anything in 3D Images with Med3D Adapter. arXiv preprint arXiv:2308.16184.&lt;/li&gt;
&lt;li&gt;Arango, S. P., Ferreira, F., Kadra, A., Hutter, F., &amp;amp; Grabocka, J. (2023). Quick-tune: Quickly learning which pretrained model to finetune and how. arXiv preprint arXiv:2306.03828.&lt;/li&gt;
&lt;li&gt;Awad, N., Doerr, F., Müller, S., Benjamins, C., &amp;amp; Hutter, F. (2021). DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR 130: 1344–1352.  &lt;a href=&#34;https://proceedings.mlr.press/v130/awad21a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://proceedings.mlr.press/v130/awad21a.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Supervisors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Natalia Oviedo Acosta&lt;/li&gt;
&lt;li&gt;Stefan Klein&lt;/li&gt;
&lt;li&gt;Martijn Starmans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Feel free to mail me if you are interested in this project or want more information!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Preprocessing Pipeline for Soft Tissue Tumor Classification</title>
      <link>https://MStarmans91.github.io/project/preprocessing/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://MStarmans91.github.io/project/preprocessing/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In medical imaging, preprocessing steps like normalization, resampling, and cropping are often applied using fixed heuristics. While standard, these static methods can limit performance in complex settings such as soft tissue tumors (STTs), where anatomical variation and acquisition inconsistencies are common.  nnU-Net [1] has shown that task-specific, well-chosen preprocessing can strongly influence performance by adapting settings to the dataset characteristics. Building on this idea, and inspired by OBELISK-Net [2], which integrates preprocessing within the network as a learnable module, this project explores a dynamic alternative: making preprocessing fully differentiable and trainable alongside the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aim&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project is to build an adaptive preprocessing pipeline for medical imaging with a focus on STTs. Instead of using fixed rules for intensity normalization, voxel spacing resampling, or spatial cropping, the project will implement these steps as learnable layers in a deep neural network. These layers will be differentiable and trained end-to-end with the downstream model, allowing them to discover the most effective image transformations in a data-driven, task-specific way. The proposed pipeline will be compared with conventional preprocessing strategies and evaluated on public medical imaging datasets, especially for rare cancers such as STTs. Note that this project is quite technical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related research:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Isensee, F., Jaeger, P.F., Full, P.M., Vollmuth, P., Maier-Hein, K.H.: nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18, 203–211 (2021)&lt;/li&gt;
&lt;li&gt;Heinrich, M. P., Oktay, O., &amp;amp; Bouteldja, N. (2019). OBELISK-Net: Fewer layers to solve 3D multi-organ segmentation with sparse deformable convolutions. Medical image analysis, 54, 1-9.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Supervisors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Natalia Oviedo Acosta&lt;/li&gt;
&lt;li&gt;Stefan Klein&lt;/li&gt;
&lt;li&gt;Martijn Starmans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Feel free to mail me if you are interested in this project or want more information!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Image to Insight; Can Vision Language Models see like Radiologists?</title>
      <link>https://MStarmans91.github.io/project/laichatgpt/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://MStarmans91.github.io/project/laichatgpt/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Diagnosing liver lesions using multiparametric MRI requires detecting complex, often subtle features - such as washout, fat content, capsular enhancement, and iron deposition - However, these features can be subtle and challenging to detect, often requiring the expertise of experienced radiologists, which limits the scalability of supervised AI approaches.&lt;/p&gt;
&lt;p&gt;With the rise of vision-language models (VLMs) like GPT-4V and DeepSeek-VL, a key question is whether such models can reliably identify and describe these features across diverse MRI sequences. This project explores the diagnostic potential of VLMs as assistive tools, with a particular focus on benchmarking their ability to &amp;ldquo;see&amp;rdquo; and reason over relevant imaging features in comparison to expert radiologist interpretations and pathology-confirmed diagnoses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aim&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The primary goal of this projectis to systematically evaluate how well these models extract and describe clinically meaningful features from annotated MRI data, and to what extent their predictions aligns with real-world diagnostic outcomes. Depending on results and interest, the project may also explore fine-tuning existing models to improve domain-specific performance, or - if performance proves reliable - developing a lightweight graphical interface to make these models more accessible and interpretable in a clinical context. This research is part of the Liver Artificial Intelligene - Consortium, a collaborative initiative focused on advancing AI models in liver imaging, bringing together interdisciplinary expertise to unlock new diagnostic possibilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related research:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.jpi.2024.100368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.jpi.2024.100368&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.ajpath.2023.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.ajpath.2023.03.012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Supervisors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frederik Hartmann&lt;/li&gt;
&lt;li&gt;Ruben Niemantsverdriet&lt;/li&gt;
&lt;li&gt;Maarten Thomeer&lt;/li&gt;
&lt;li&gt;Stefan Klein&lt;/li&gt;
&lt;li&gt;Martijn Starmans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Feel free to mail me if you are interested in this project or want more information!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
